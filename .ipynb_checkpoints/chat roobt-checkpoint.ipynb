{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30ad1ffb-bc84-4397-8098-2d14deb3ac39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "class Conversation2:\n",
    "    def __init__(self, prompt, num_of_round):\n",
    "        self.prompt = prompt\n",
    "        self.num_of_round = num_of_round\n",
    "        self.messages = []\n",
    "        self.messages.append({\"role\": \"system\", \"content\": self.prompt})\n",
    "\n",
    "    def ask(self, question):\n",
    "        try:\n",
    "            self.messages.append( {\"role\": \"user\", \"content\": question})\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=self.messages,\n",
    "                temperature=0.5,\n",
    "                max_tokens=2048,\n",
    "                top_p=1,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return e\n",
    "\n",
    "        message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        num_of_tokens = response['usage']['total_tokens']\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": message})\n",
    "        \n",
    "        if len(self.messages) > self.num_of_round*2 + 1:\n",
    "            del self.messages[1:3]\n",
    "        return message, num_of_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37de1ae2-a44e-41c5-be36-c2df683bf3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Conversation2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m你是一個英文老师，你可以和别人练习英文， 纠正语法错误，并且你的回答需要滿足以下條件:\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124m1. 你的回答必须是中文\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124m2. 回答限制在100个字以内\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m conv2 \u001b[38;5;241m=\u001b[39m \u001b[43mConversation2\u001b[49m(prompt, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      5\u001b[0m question1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m你是谁？\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser : \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m question1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Conversation2' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"你是一個英文老师，你可以和别人练习英文， 纠正语法错误，并且你的回答需要滿足以下條件:\n",
    "1. 你的回答必须是中文\n",
    "2. 回答限制在100个字以内\"\"\"\n",
    "conv2 = Conversation2(prompt, 2)\n",
    "question1 = \"你是谁？\"\n",
    "print(\"User : %s\" % question1)\n",
    "answer,num_of_tokens=conv2.ask(question1)\n",
    "print(\"Assistant : {%s} 消耗的token數量是： %d \\n\" % (answer,num_of_tokens))\n",
    "\n",
    "question2 = \"請問數據分析有什麽方法\"\n",
    "print(\"User : %s\" % question2)\n",
    "answer,num_of_tokens=conv2.ask(question2)\n",
    "print(\"Assistant : {%s} 消耗的token數量是： %d \\n\" % (answer,num_of_tokens))\n",
    "\n",
    "question3 = \"就第一個方法展開介紹一下？\"\n",
    "print(\"User : %s\" % question3)\n",
    "answer,num_of_tokens=conv2.ask(question3)\n",
    "print(\"Assistant : {%s} 消耗的token數量是： %d \\n\" % (answer,num_of_tokens))\n",
    "\n",
    "question4 = \"就第二個方法展開介紹一下？\"\n",
    "print(\"User : %s\" % question4)\n",
    "answer,num_of_tokens=conv2.ask(question4)\n",
    "print(\"Assistant : {%s} 消耗的token數量是： %d \\n\" % (answer,num_of_tokens))\n",
    "\n",
    "question5 = \"就第9個方法展開介紹一下？\"\n",
    "print(\"User : %s\" % question5)\n",
    "answer,num_of_tokens=conv2.ask(question5)\n",
    "print(\"Assistant : {%s} 消耗的token數量是： %d \\n\" % (answer,num_of_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd0f9b55-d74d-43d8-9fc6-9ee8471844da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "class Conversation:\n",
    "    def __init__(self, prompt, num_of_round):\n",
    "        self.prompt = prompt\n",
    "        self.num_of_round = num_of_round\n",
    "        self.messages = []\n",
    "        self.messages.append({\"role\": \"system\", \"content\": self.prompt})\n",
    "\n",
    "    def ask(self, question):\n",
    "        try:\n",
    "            self.messages.append({\"role\": \"user\", \"content\": question})\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=self.messages,\n",
    "                temperature=0.5,\n",
    "                max_tokens=2048,\n",
    "                top_p=1,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return e\n",
    "\n",
    "        message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": message})\n",
    "\n",
    "        if len(self.messages) > self.num_of_round*2 + 1:\n",
    "            del self.messages[1:3] \n",
    "        return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b898a8-36c6-4bc0-845b-2f0148b23308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "prompt = \"\"\"你是一個數據分析師，你會用中文教人學數據分析，并且你的回答需要滿足以下條件:\n",
    "1. 你的回答必须是中文\n",
    "2. 回答限制在100个字以内\"\"\"\n",
    "\n",
    "conv = Conversation(prompt, 2)\n",
    "\n",
    "def answer(question, history=[]):\n",
    "    history.append(question)\n",
    "    response = conv.ask(question)\n",
    "    history.append(response)\n",
    "    responses = [(u,b) for u,b in zip(history[::2], history[1::2])]\n",
    "    return responses, history\n",
    "\n",
    "with gr.Blocks(css=\"#chatbot{height:300px} .overflow-y-auto{height:500px}\") as demo:\n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
    "    state = gr.State([])\n",
    "\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n",
    "\n",
    "    txt.submit(answer, [txt, state], [chatbot, state])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "098d87f0-a01d-455b-9d8c-c3762452607a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.cognitiveservices.speech.ResultFuture at 0x13124b3ff10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "# This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "speech_config = speechsdk.SpeechConfig(subscription=os.environ.get('SPEECH_KEY'), region=os.environ.get('SPEECH_REGION'))\n",
    "audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)\n",
    "\n",
    "# The language of the voice that speaks.\n",
    "speech_config.speech_synthesis_voice_name='zh-CN-XiaohanNeural'\n",
    "\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "text = \"今天天气真不错，ChatGPT真好用。\"\n",
    "\n",
    "speech_synthesizer.speak_text_async(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05ae6d40-0f74-4a78-81fb-ff136824599b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ssml = \"\"\"<speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\"\n",
    "       xmlns:mstts=\"https://www.w3.org/2001/mstts\" xml:lang=\"en-US\">\n",
    "    <voice name=\"en-US-JennyNeural\">\n",
    "        <mstts:express-as style=\"excited\">\n",
    "            That'd be just amazing!\n",
    "        </mstts:express-as>\n",
    "        <mstts:express-as style=\"friendly\">\n",
    "            What's next?\n",
    "        </mstts:express-as>\n",
    "    </voice>\n",
    "</speak>\"\"\"\n",
    "\n",
    "speech_synthesis_result = speech_synthesizer.speak_ssml_async(ssml).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5125a85-6154-4b7e-b7c4-31814c095673",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "speech_config.speech_synthesis_language='zh-CN'\n",
    "speech_config.speech_synthesis_voice_name='zh-CN-XiaohanNeural'\n",
    "\n",
    "audio_config = speechsdk.audio.AudioOutputConfig(filename=\".tts.wav\")\n",
    "\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "ssml = \"\"\"<speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\"\n",
    "       xmlns:mstts=\"https://www.w3.org/2001/mstts\" xml:lang=\"en-US\">\n",
    "    <voice name=\"en-US-JennyNeural\">\n",
    "        <mstts:express-as style=\"excited\">\n",
    "            That'd be just amazing!\n",
    "        </mstts:express-as>\n",
    "        <mstts:express-as style=\"friendly\">\n",
    "            What's next?\n",
    "        </mstts:express-as>\n",
    "    </voice>\n",
    "</speak>\"\"\"\n",
    "\n",
    "speech_synthesis_result = speech_synthesizer.speak_ssml_async(ssml).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50ee578b-24f1-4f38-aeaa-b8a48fbd2402",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ariel\\anaconda3\\envs\\py310\\lib\\site-packages\\gradio\\processing_utils.py:239: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ariel\\anaconda3\\envs\\py310\\lib\\site-packages\\gradio\\routes.py\", line 394, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\Ariel\\anaconda3\\envs\\py310\\lib\\site-packages\\gradio\\blocks.py\", line 1075, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\Ariel\\anaconda3\\envs\\py310\\lib\\site-packages\\gradio\\blocks.py\", line 884, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\Users\\Ariel\\anaconda3\\envs\\py310\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"C:\\Users\\Ariel\\anaconda3\\envs\\py310\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\Ariel\\anaconda3\\envs\\py310\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\Ariel\\AppData\\Local\\Temp\\ipykernel_16568\\761647423.py\", line 67, in process_audio\n",
      "    text = transcribe(audio)\n",
      "  File \"C:\\Users\\Ariel\\AppData\\Local\\Temp\\ipykernel_16568\\761647423.py\", line 61, in transcribe\n",
      "    os.rename(audio, audio + '.wav')\n",
      "TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ariel\\anaconda3\\envs\\py310\\lib\\site-packages\\gradio\\routes.py\", line 394, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\Ariel\\anaconda3\\envs\\py310\\lib\\site-packages\\gradio\\blocks.py\", line 1075, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\Ariel\\anaconda3\\envs\\py310\\lib\\site-packages\\gradio\\blocks.py\", line 884, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\Users\\Ariel\\anaconda3\\envs\\py310\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"C:\\Users\\Ariel\\anaconda3\\envs\\py310\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\Ariel\\anaconda3\\envs\\py310\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\Ariel\\AppData\\Local\\Temp\\ipykernel_16568\\761647423.py\", line 67, in process_audio\n",
      "    text = transcribe(audio)\n",
      "  File \"C:\\Users\\Ariel\\AppData\\Local\\Temp\\ipykernel_16568\\761647423.py\", line 61, in transcribe\n",
      "    os.rename(audio, audio + '.wav')\n",
      "TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\n"
     ]
    }
   ],
   "source": [
    "import openai, os\n",
    "import gradio as gr\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "class Conversation:\n",
    "    def __init__(self, prompt, num_of_round):\n",
    "        self.prompt = prompt\n",
    "        self.num_of_round = num_of_round\n",
    "        self.messages = []\n",
    "        self.messages.append({\"role\": \"system\", \"content\": self.prompt})\n",
    "\n",
    "    def ask(self, question):\n",
    "        try:\n",
    "            self.messages.append({\"role\": \"user\", \"content\": question})\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=self.messages,\n",
    "                temperature=0.5,\n",
    "                max_tokens=2048,\n",
    "                top_p=1,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return e\n",
    "\n",
    "        message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": message})\n",
    "\n",
    "        if len(self.messages) > self.num_of_round*2 + 1:\n",
    "            del self.messages[1:3] \n",
    "        return message\n",
    "\n",
    "prompt = \"\"\"你是一個英文老师，你可以和别人练习口语，纠正语法，并且你的回答需要滿足以下條件:\n",
    "1. 回答限制在100个字以内\"\"\"\n",
    "conv = Conversation(prompt, 2)\n",
    "\n",
    "\n",
    "speech_config = speechsdk.SpeechConfig(subscription=os.environ.get('SPEECH_KEY'), region=os.environ.get('SPEECH_REGION'))\n",
    "audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)\n",
    "\n",
    "# The language of the voice that speaks.\n",
    "speech_config.speech_synthesis_language='en-US'\n",
    "speech_config.speech_synthesis_voice_name='en-US-JennyNeural'\n",
    "\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "def play_voice(text):\n",
    "    speech_synthesizer.speak_text_async(text)\n",
    "\n",
    "def answer(question, history=[]):\n",
    "    history.append(question)\n",
    "    response = conv.ask(question)\n",
    "    history.append(response)\n",
    "    play_voice(response)\n",
    "    responses = [(u,b) for u,b in zip(history[::2], history[1::2])]\n",
    "    return responses, history\n",
    "    \n",
    "def transcribe(audio):\n",
    "    os.rename(audio, audio + '.wav')\n",
    "    audio_file = open(audio + '.wav', \"rb\")\n",
    "    transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "    return transcript['text']  \n",
    "\n",
    "def process_audio(audio, history=[]):\n",
    "    text = transcribe(audio)\n",
    "    return answer(text, history)   \n",
    "\n",
    "with gr.Blocks(css=\"#chatbot{height:800px} .overflow-y-auto{height:800px}\") as demo:\n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
    "    state = gr.State([])\n",
    "\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n",
    "        \n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(source=\"microphone\", type=\"filepath\")\n",
    "        \n",
    "    txt.submit(answer, [txt, state], [chatbot, state])\n",
    "    audio.change(process_audio, [audio, state], [chatbot, state])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22a2c49f-3647-4a44-9ad6-7f0db33e6839",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot construct SpeechConfig with the given arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcognitiveservices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspeech\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspeechsdk\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m speech_config \u001b[38;5;241m=\u001b[39m \u001b[43mspeechsdk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSpeechConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubscription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSPEECH_KEY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSPEECH_REGION\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m audio_config \u001b[38;5;241m=\u001b[39m speechsdk\u001b[38;5;241m.\u001b[39maudio\u001b[38;5;241m.\u001b[39mAudioOutputConfig(use_default_speaker\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# The language of the voice that speaks.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\azure\\cognitiveservices\\speech\\speech.py:84\u001b[0m, in \u001b[0;36mSpeechConfig.__init__\u001b[1;34m(self, subscription, region, endpoint, host, auth_token, speech_recognition_language)\u001b[0m\n\u001b[0;32m     82\u001b[0m     _call_hr_fn(fn\u001b[38;5;241m=\u001b[39m_sdk_lib\u001b[38;5;241m.\u001b[39mspeech_config_from_host, \u001b[38;5;241m*\u001b[39m[ctypes\u001b[38;5;241m.\u001b[39mbyref(handle), c_host, c_subscription])\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(generic_error_message)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__handle \u001b[38;5;241m=\u001b[39m _Handle(handle, _sdk_lib\u001b[38;5;241m.\u001b[39mspeech_config_is_handle_valid, _sdk_lib\u001b[38;5;241m.\u001b[39mspeech_config_release)\n\u001b[0;32m     86\u001b[0m prop_handle \u001b[38;5;241m=\u001b[39m _spx_handle(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot construct SpeechConfig with the given arguments"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "# This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "speech_config = speechsdk.SpeechConfig(subscription=os.environ.get('SPEECH_KEY'), region=os.environ.get('SPEECH_REGION'))\n",
    "audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)\n",
    "\n",
    "# The language of the voice that speaks.\n",
    "speech_config.speech_synthesis_voice_name='en-US-JennyNeural'\n",
    "\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "# Get text from the console and synthesize to the default speaker.\n",
    "print(\"Enter some text that you want to speak >\")\n",
    "text = input()\n",
    "\n",
    "speech_synthesis_result = speech_synthesizer.speak_text_async(text).get()\n",
    "\n",
    "if speech_synthesis_result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "    print(\"Speech synthesized for text [{}]\".format(text))\n",
    "elif speech_synthesis_result.reason == speechsdk.ResultReason.Canceled:\n",
    "    cancellation_details = speech_synthesis_result.cancellation_details\n",
    "    print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n",
    "    if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "        if cancellation_details.error_details:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and region values?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1db3e811-6b44-41df-a974-bf27b1df1e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPEECH_KEY 的值为 None\n",
      "SPEECH_REGION 的值为 None\n",
      "openai_key 的值为 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\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# 获取 SPEECH_KEY 和 SPEECH_REGION 的值\n",
    "speech_key = os.environ.get('SPEECH_KEY')\n",
    "speech_region = os.environ.get('SPEECH_REGION')\n",
    "openai_key=os.environ.get('OPENAI_API_KEY')\n",
    "# 检查是否设置成功\n",
    "\n",
    "print(f'SPEECH_KEY 的值为 {speech_key}')\n",
    "print(f'SPEECH_REGION 的值为 {speech_region}')\n",
    "print(f'openai_key 的值为 {openai_key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50393c70-d2f5-4374-a96d-4ba9ef29387f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'tlk_wu7QXXhXhqRX8-1gtRnGN', 'created_at': '2023-04-29T16:42:12.576Z', 'created_by': 'google-oauth2|112769396708518126477', 'status': 'created', 'object': 'talk'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def generate_talk(input, avatar_url, \n",
    "                  voice_type = \"microsoft\", \n",
    "                  voice_id = \"zh-CN-XiaomoNeural\", \n",
    "                  api_key = os.environ.get('DID_API_KEY')):\n",
    "    url = \"https://api.d-id.com/talks\"\n",
    "    payload = {\n",
    "        \"script\": {\n",
    "            \"type\": \"text\",\n",
    "            \"provider\": {\n",
    "                \"type\": voice_type,\n",
    "                \"voice_id\": voice_id\n",
    "            },\n",
    "            \"ssml\": \"false\",\n",
    "            \"input\": input\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"fluent\": \"false\",\n",
    "            \"pad_audio\": \"0.0\"\n",
    "        },\n",
    "        \"source_url\": avatar_url\n",
    "    }\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"authorization\": \"Basic \" + api_key\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "avatar_url = \"https://cdn.discordapp.com/attachments/1065596492796153856/1095617463112187984/John_Carmack_Potrait_668a7a8d-1bb0-427d-8655-d32517f6583d.png\"\n",
    "text = \"今天天气真不错呀。\"\n",
    "\n",
    "response = generate_talk(input=text, avatar_url=avatar_url)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "717c1ac9-3b36-4477-965c-a6616771580b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user': {'features': ['stitch', 'api-keys:write', 'talks'], 'stripe_customer_id': 'cus_NnsMtcSVvauHJO', 'authorizer': 'basic', 'owner_id': 'google-oauth2|112769396708518126477', 'id': 'google-oauth2|112769396708518126477', 'plan': 'deid-lite', 'email': 'zoey.liu.zy@gmail.com'}, 'metadata': {'driver_url': 'bank://lively/driver-01/original', 'mouth_open': False, 'num_faces': 1, 'num_frames': 52, 'processing_fps': 19.654608060657143, 'resolution': [512, 512], 'size_kib': 219.716796875}, 'audio_url': 'https://d-id-talks-prod.s3.us-west-2.amazonaws.com/google-oauth2%7C112769396708518126477/tlk_wu7QXXhXhqRX8-1gtRnGN/microsoft.wav?AWSAccessKeyId=AKIA5CUMPJBIK65W6FGA&Expires=1682872933&Signature=DC7Nk3xZUFAPlx5krm0K3Zuaes0%3D&X-Amzn-Trace-Id=Root%3D1-644d48e3-7485c37f09f836b57150f326%3BParent%3D07be6197763fd90a%3BSampled%3D0%3BLineage%3Da08e19fe%3A0', 'created_at': '2023-04-29T16:42:12.576Z', 'face': {'mask_confidence': -1, 'detection': [302, 224, 754, 795], 'overlap': 'no', 'size': 845, 'top_left': [106, 87], 'face_id': 0, 'detect_confidence': 0.9999815225601196}, 'config': {'stitch': False, 'pad_audio': 0, 'align_driver': True, 'sharpen': True, 'reduce_noise': False, 'auto_match': True, 'normalization_factor': 1, 'logo': {'url': 'd-id-logo', 'position': [0, 0]}, 'motion_factor': 1, 'result_format': '.mp4', 'fluent': False, 'align_expand_factor': 0.3}, 'source_url': 'https://d-id-talks-prod.s3.us-west-2.amazonaws.com/google-oauth2%7C112769396708518126477/tlk_wu7QXXhXhqRX8-1gtRnGN/source/John_Carmack_Potrait_668a7a8d-1bb0-427d-8655-d32517f6583d.png?AWSAccessKeyId=AKIA5CUMPJBIK65W6FGA&Expires=1682872933&Signature=5p%2Bn0MrCIzHl6V3pzKjBGDqLt6w%3D&X-Amzn-Trace-Id=Root%3D1-644d48e3-7485c37f09f836b57150f326%3BParent%3D07be6197763fd90a%3BSampled%3D0%3BLineage%3Da08e19fe%3A0', 'created_by': 'google-oauth2|112769396708518126477', 'status': 'done', 'driver_url': 'bank://lively/', 'modified_at': '2023-04-29T16:42:16.370Z', 'user_id': 'google-oauth2|112769396708518126477', 'result_url': 'https://d-id-talks-prod.s3.us-west-2.amazonaws.com/google-oauth2%7C112769396708518126477/tlk_wu7QXXhXhqRX8-1gtRnGN/1682786533578.mp4?AWSAccessKeyId=AKIA5CUMPJBIK65W6FGA&Expires=1682872936&Signature=ql%2BAW%2F9lKGK8l45RQP1Z9pNsUxo%3D', 'id': 'tlk_wu7QXXhXhqRX8-1gtRnGN', 'duration': 3, 'started_at': '2023-04-29T16:42:13.640'}\n"
     ]
    }
   ],
   "source": [
    "def get_a_talk(id, api_key = os.environ.get('DID_API_KEY')):\n",
    "    url = \"https://api.d-id.com/talks/\" + id\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"authorization\": \"Basic \"+api_key\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "talk = get_a_talk(response['id'])\n",
    "print(talk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cddf689-e23a-4ebd-a237-7bcac7a09e12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <video width=\"640\" height=\"480\" controls>\n",
       "        <source src=\"https://d-id-talks-prod.s3.us-west-2.amazonaws.com/google-oauth2%7C112769396708518126477/tlk_wu7QXXhXhqRX8-1gtRnGN/1682786533578.mp4?AWSAccessKeyId=AKIA5CUMPJBIK65W6FGA&Expires=1682872936&Signature=ql%2BAW%2F9lKGK8l45RQP1Z9pNsUxo%3D\" type=\"video/mp4\">\n",
       "    Your browser does not support the video tag.\n",
       "    </video>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "def play_mp4_video(url):\n",
    "    video_tag = f\"\"\"\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "        <source src=\"{url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "    </video>\n",
    "    \"\"\"\n",
    "    return HTML(video_tag)\n",
    "result_url = talk['result_url']\n",
    "play_mp4_video(result_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adcb2530-4798-4ea0-b13d-53b423067ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ariel\\anaconda3\\envs\\py310\\lib\\site-packages\\gradio\\deprecation.py:43: UserWarning: You have unused kwarg parameters in HTML, please remove them: {'live': False}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai, os, time, requests\n",
    "import gradio as gr\n",
    "from gradio import HTML\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "class Conversation:\n",
    "    def __init__(self, prompt, num_of_round):\n",
    "        self.prompt = prompt\n",
    "        self.num_of_round = num_of_round\n",
    "        self.messages = []\n",
    "        self.messages.append({\"role\": \"system\", \"content\": self.prompt})\n",
    "\n",
    "    def ask(self, question):\n",
    "        try:\n",
    "            self.messages.append({\"role\": \"user\", \"content\": question})\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=self.messages,\n",
    "                temperature=0.5,\n",
    "                max_tokens=2048,\n",
    "                top_p=1,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return e\n",
    "\n",
    "        message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": message})\n",
    "\n",
    "        if len(self.messages) > self.num_of_round*2 + 1:\n",
    "            del self.messages[1:3] \n",
    "        return message\n",
    "\n",
    "prompt = \"\"\"你是一個恒生银行的智能客服，你除了可以回答一般的问题在回答与银行相关或者数据相关的问题时请回答的专业一点:\n",
    "1. 回答限制在100个字以内\"\"\"\n",
    "conv = Conversation(prompt, 2)\n",
    "\n",
    "avatar_url = \"blob:https://discord.com/4577ecce-8bcd-4f1c-a6ad-22cd969e33a1\"\n",
    "\n",
    "def generate_talk(input, avatar_url, \n",
    "                  voice_type = \"microsoft\", \n",
    "                  voice_id = \"zh-CN-XiaomoNeural\", \n",
    "                  api_key = os.environ.get('DID_API_KEY')):\n",
    "    url = \"https://api.d-id.com/talks\"\n",
    "    payload = {\n",
    "        \"script\": {\n",
    "            \"type\": \"text\",\n",
    "            \"provider\": {\n",
    "                \"type\": voice_type,\n",
    "                \"voice_id\": voice_id\n",
    "            },\n",
    "            \"ssml\": \"false\",\n",
    "            \"input\": input\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"fluent\": \"false\",\n",
    "            \"pad_audio\": \"0.0\"\n",
    "        },\n",
    "        \"source_url\": avatar_url\n",
    "    }\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"authorization\": \"Basic \" + api_key\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "\n",
    "def get_a_talk(id, api_key = os.environ.get('DID_API_KEY')):\n",
    "    url = \"https://api.d-id.com/talks/\" + id\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"authorization\": \"Basic \"+api_key\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "\n",
    "def get_mp4_video(input, avatar_url=avatar_url):\n",
    "    response = generate_talk(input=input, avatar_url=avatar_url)\n",
    "    talk = get_a_talk(response['id'])\n",
    "    video_url = \"\"\n",
    "    index = 0\n",
    "    while index < 30:\n",
    "        index += 1\n",
    "        if 'result_url' in talk:    \n",
    "            video_url = talk['result_url']\n",
    "            return video_url\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "            talk = get_a_talk(response['id'])\n",
    "    return video_url\n",
    "\n",
    "def answer(question, history=[]):\n",
    "    if input is not None:\n",
    "        history.append(question)\n",
    "        response = conv.ask(question)\n",
    "        video_url = get_mp4_video(input=response, avatar_url=avatar_url)\n",
    "        video_html = f\"\"\"<video width=\"320\" height=\"240\" controls autoplay><source src=\"{video_url}\" type=\"video/mp4\"></video>\"\"\"\n",
    "        history.append(response)\n",
    "        responses = [(u,b) for u,b in zip(history[::2], history[1::2])]\n",
    "        return responses, video_html, history\n",
    "    else:\n",
    "        video_html = f'< img src=\"{avatar_url}\" width=\"320\" height=\"240\" alt=\"John Carmack\">'\n",
    "        responses = [(u,b) for u,b in zip(history[::2], history[1::2])]\n",
    "        return responses, video_html, history        \n",
    "    \n",
    "\n",
    "def transcribe(audio):\n",
    "    os.rename(audio, audio + '.wav')\n",
    "    audio_file = open(audio + '.wav', \"rb\")\n",
    "    transcript = openai.Audio.transcribe(\"whisper-1\", audio_file, prompt=\"这是一段中文的问题。\")\n",
    "    return transcript['text']    \n",
    "\n",
    "def process_audio(audio, history=[]):\n",
    "    if audio is not None:\n",
    "        text = transcribe(audio)\n",
    "        return answer(text, history)\n",
    "    else:\n",
    "        text = None\n",
    "        return answer(text, history)\n",
    "\n",
    "with gr.Blocks(css=\"#chatbot{height:500px} .overflow-y-auto{height:500px}\") as demo:\n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
    "    state = gr.State([])\n",
    "\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n",
    "        \n",
    "    with gr.Row():\n",
    "        audio = gr.Audio(source=\"microphone\", type=\"filepath\")\n",
    "\n",
    "    with gr.Row():\n",
    "        video = gr.HTML(f'< img src=\"{avatar_url}\" width=\"320\" height=\"240\" alt=\"John Carmack\">', live=False)\n",
    "\n",
    "    txt.submit(answer, [txt, state], [chatbot, video, state])\n",
    "    audio.change(process_audio, [audio, state], [chatbot, video, state])\n",
    "    \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f752b9-127f-476c-b73d-a0d80b9626a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
